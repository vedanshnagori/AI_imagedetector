{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083d995e-3296-4301-81cf-528e9878d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import cv2 as cv\n",
    "import os\n",
    "import random\n",
    "import PIL\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c955c7eb-0b8c-4843-84a0-9661454c3b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r'D:\\Vista 25\\Dataset\\vista-25\\dataset'\n",
    "sets = ['train', 'test']\n",
    "categories = ['real', 'fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb694f73-7071-4701-8da3-3a3937780dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 48000\n",
      "Total test images: 12000\n",
      "Sample train image: [('D:\\\\Vista 25\\\\Dataset\\\\vista-25\\\\dataset\\\\train\\\\real\\\\0001.jpg', 0), ('D:\\\\Vista 25\\\\Dataset\\\\vista-25\\\\dataset\\\\train\\\\real\\\\0002.jpg', 0), ('D:\\\\Vista 25\\\\Dataset\\\\vista-25\\\\dataset\\\\train\\\\real\\\\0003.jpg', 0), ('D:\\\\Vista 25\\\\Dataset\\\\vista-25\\\\dataset\\\\train\\\\real\\\\0004.jpg', 0), ('D:\\\\Vista 25\\\\Dataset\\\\vista-25\\\\dataset\\\\train\\\\real\\\\0005.jpg', 0)]\n",
      "Sample test image: ['D:\\\\Vista 25\\\\Dataset\\\\vista-25\\\\dataset\\\\test\\\\0.jpg', 'D:\\\\Vista 25\\\\Dataset\\\\vista-25\\\\dataset\\\\test\\\\1.jpg', 'D:\\\\Vista 25\\\\Dataset\\\\vista-25\\\\dataset\\\\test\\\\10.jpg', 'D:\\\\Vista 25\\\\Dataset\\\\vista-25\\\\dataset\\\\test\\\\100.jpg', 'D:\\\\Vista 25\\\\Dataset\\\\vista-25\\\\dataset\\\\test\\\\1000.jpg']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_size = 64\n",
    "all_train_images = []\n",
    "test_images = []\n",
    "random_seed = 42\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Collect all image paths with their category labels\n",
    "for dataset in sets:  # Renamed loop variable to avoid conflict\n",
    "    path = os.path.join(data, dataset)  # Define the correct dataset path\n",
    "    \n",
    "    if dataset == 'test':  # Corrected string comparison\n",
    "        for img in os.listdir(path):\n",
    "            img_path = os.path.join(path, img)\n",
    "            test_images.append(img_path)\n",
    "        continue  # Skip further execution for test set\n",
    "    \n",
    "    for category in categories:\n",
    "        category_path = os.path.join(path, category)  # Correct training path\n",
    "        class_idx = categories.index(category)\n",
    "        for img in os.listdir(category_path):\n",
    "            img_path = os.path.join(category_path, img)\n",
    "            all_train_images.append((img_path, class_idx))  # Use the correct list\n",
    "\n",
    "# Print some sample outputs for verification\n",
    "print(f\"Total training images: {len(all_train_images)}\")\n",
    "print(f\"Total test images: {len(test_images)}\")\n",
    "print(\"Sample train image:\", all_train_images[:5])\n",
    "print(\"Sample test image:\", test_images[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f33085-9aea-41f0-ae8e-52ade654114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4cad6e0-d31d-4114-b92f-8cacfd88f97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train images...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:  33%|███████████████████▏                                      | 15903/48000 [11:42<20:48, 25.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping corrupted image: D:\\Vista 25\\Dataset\\vista-25\\dataset\\train\\real\\11957.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|█████████████████████████████████████████████████████████▉| 47999/48000 [36:05<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing train. Valid images: 47999, Corrupted: 1\n",
      "\n",
      "\n",
      "Processing test images...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test:  79%|███████████████████████████████████████████████▎            | 9466/12000 [07:22<01:07, 37.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping corrupted image: D:\\Vista 25\\Dataset\\vista-25\\dataset\\test\\7711.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████████████████████████████████████████████████████▉| 11999/12000 [09:22<00:00, 21.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished processing test. Valid images: 11999, Corrupted: 1\n",
      "\n",
      "Final training dataset size: 47999 images\n",
      "Final test dataset size: 11999 images\n",
      "Corrupted training images: [15901]\n",
      "Corrupted test images: [9459]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize storage\n",
    "training_data = []\n",
    "test_data = []\n",
    "corrupted_train_indices = []\n",
    "corrupted_test_indices = []\n",
    "\n",
    "# Function to process images\n",
    "def process_images(image_list, dataset_name):\n",
    "    processed_data = []\n",
    "    corrupted_indices = []\n",
    "\n",
    "    print(f\"\\nProcessing {dataset_name} images...\\n\")\n",
    "    with tqdm(total=len(image_list), desc=f\"Processing {dataset_name}\") as progress_bar:\n",
    "        for idx, item in enumerate(image_list):\n",
    "            try:\n",
    "                if dataset_name == \"train\":\n",
    "                    img_path, class_idx = item  # Training data has labels\n",
    "                else:\n",
    "                    img_path = item  # Test data has no labels\n",
    "                \n",
    "                img_array = cv.imread(img_path)  \n",
    "                \n",
    "                # Check for corrupted images\n",
    "                if img_array is None:\n",
    "                    print(f\"Skipping corrupted image: {img_path}\")\n",
    "                    corrupted_indices.append(idx)\n",
    "                    continue\n",
    "                \n",
    "                # Ensure the image has 3 channels (RGB)\n",
    "                if img_array.shape[-1] != 3:\n",
    "                    print(f\"Skipping non-RGB image: {img_path}\")\n",
    "                    corrupted_indices.append(idx)\n",
    "                    continue\n",
    "                \n",
    "                # Resize and normalize\n",
    "                new_array = cv.resize(img_array, (img_size, img_size)) / 255.0  \n",
    "\n",
    "                # Append processed data\n",
    "                if dataset_name == \"train\":\n",
    "                    processed_data.append([new_array, class_idx])\n",
    "                else:\n",
    "                    processed_data.append(new_array)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {str(e)}\")\n",
    "                corrupted_indices.append(idx)\n",
    "                continue\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    print(f\"\\nFinished processing {dataset_name}. Valid images: {len(processed_data)}, Corrupted: {len(corrupted_indices)}\\n\")\n",
    "    return processed_data, corrupted_indices\n",
    "\n",
    "# Process Training Data\n",
    "training_data, corrupted_train_indices = process_images(all_train_images, \"train\")\n",
    "\n",
    "# Process Test Data\n",
    "test_data, corrupted_test_indices = process_images(test_images, \"test\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Final training dataset size: {len(training_data)} images\")\n",
    "print(f\"Final test dataset size: {len(test_data)} images\")\n",
    "print(f\"Corrupted training images: {corrupted_train_indices}\")\n",
    "print(f\"Corrupted test images: {corrupted_test_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4989fdfd-be37-4180-9359-367a67ba7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f2e0550-513e-45d2-a962-128641412b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first image: (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of first image:\", training_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b36c30b-5802-43d5-a00d-8e84760e4fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (38399, 64, 64, 3)\n",
      "y_train shape: (38399,)\n",
      "x_val shape: (9600, 64, 64, 3)\n",
      "y_val shape: (9600,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Extract features and labels\n",
    "X = np.array([item[0] for item in training_data], dtype=np.float32)\n",
    "y = np.array([item[1] for item in training_data], dtype=np.int32)\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_val shape: {x_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4813c4d7-69ec-44c1-a578-1ced5dce8698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "46860ab0-c0d2-459d-9824-d96ad5044831",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "3c5e29d7-2a3b-4152-bf83-6f5a5f5c97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(64,64,3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "883559ca-3e03-4061-b3f3-7ce2b0155b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer: False\n",
      "block1_conv1: False\n",
      "block1_conv2: False\n",
      "block1_pool: False\n",
      "block2_conv1: False\n",
      "block2_conv2: False\n",
      "block2_pool: False\n",
      "block3_conv1: False\n",
      "block3_conv2: False\n",
      "block3_conv3: False\n",
      "block3_pool: False\n",
      "block4_conv1: False\n",
      "block4_conv2: False\n",
      "block4_conv3: False\n",
      "block4_pool: False\n",
      "block5_conv1: True\n",
      "block5_conv2: True\n",
      "block5_conv3: True\n",
      "block5_pool: True\n"
     ]
    }
   ],
   "source": [
    "    base_model.trainable = False\n",
    "    \n",
    "    # Then unfreeze only the last 5 layers\n",
    "    for layer in base_model.layers[-4:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Print layer names and their trainable status\n",
    "    for layer in base_model.layers:\n",
    "        print(f\"{layer.name}: {layer.trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "defc62c2-d178-4ee6-a5b7-b784106c3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    MaxPooling2D(pool_size=(2, 2)),  # Replace GlobalAveragePooling2D with MaxPooling2D\n",
    "    tf.keras.layers.Flatten(),       # Flatten the output for Dense layers\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Assuming binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "5e13c4d4-3722-418e-9f71-a2778fbd14fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │          \u001b[38;5;34m16,416\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,731,137</span> (56.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,731,137\u001b[0m (56.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,095,873</span> (27.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,095,873\u001b[0m (27.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,635,264</span> (29.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,635,264\u001b[0m (29.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "29e1fdd9-5f8c-412b-8c72-1e8988c54a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "11709cf1-c33c-4162-b32f-60d856871d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 227ms/step - accuracy: 0.7057 - loss: 0.5750 - val_accuracy: 0.7867 - val_loss: 0.4545 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 257ms/step - accuracy: 0.7846 - loss: 0.4673 - val_accuracy: 0.8081 - val_loss: 0.4241 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 248ms/step - accuracy: 0.8196 - loss: 0.4135 - val_accuracy: 0.8083 - val_loss: 0.4134 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 257ms/step - accuracy: 0.8352 - loss: 0.3803 - val_accuracy: 0.8173 - val_loss: 0.4352 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 253ms/step - accuracy: 0.8505 - loss: 0.3458 - val_accuracy: 0.8165 - val_loss: 0.4337 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.1,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=lambda x: tf.image.random_contrast(x, lower=0.8, upper=1.2),\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the generator on your training data\n",
    "datagen.fit(x_train)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,  # Replace with your training data\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_data=(x_val, y_val),  # Replace with your validation data\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7075977-42f4-4832-a59f-9d6e210119a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model5.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "4dd43f48-3685-4ad1-95c3-f729a825bfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 142ms/step\n",
      "\n",
      "Predictions saved to test1.csv\n",
      "Total images: 12000\n",
      "Valid predictions: 11999\n",
      "Corrupted images: 1\n",
      "\n",
      "Corrupted image indices: [9459]\n",
      "These images were marked with prediction value: 0.5\n",
      "\n",
      "First few predictions:\n",
      "   image_id        label\n",
      "0         0  [0.6472758]\n",
      "1         1  [0.5080768]\n",
      "2         2  [0.5670067]\n",
      "3         3  [0.6922663]\n",
      "4         4  [0.7273329]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def predict_and_save(model, test_data, corrupted_test_indices, total_images=12000):\n",
    "    # Get raw predictions \n",
    "    predictions = model.predict(np.array(test_data), verbose=1)\n",
    "    \n",
    "    # Apply sigmoid\n",
    "    predicted_classes = sigmoid(predictions)\n",
    "    \n",
    "    all_predictions = []\n",
    "    valid_idx = 0 \n",
    "    \n",
    "    # Loop through all possible indices\n",
    "    for i in range(total_images):\n",
    "        if i in corrupted_test_indices:\n",
    "            prediction = 0.5\n",
    "        else:\n",
    "            # For valid images, use the prediction from our model\n",
    "            prediction = predicted_classes[valid_idx]\n",
    "            valid_idx += 1\n",
    "        \n",
    "        all_predictions.append(prediction)\n",
    "    \n",
    "    # Create DataFrame with predictions\n",
    "    df = pd.DataFrame({\n",
    "        'image_id': [i for i in range(total_images)],\n",
    "        'label': all_predictions\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('test1.csv', index=False)\n",
    "    print(\"\\nPredictions saved to test1.csv\")\n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Valid predictions: {len(test_data)}\")\n",
    "    print(f\"Corrupted images: {len(corrupted_test_indices)}\")\n",
    "    \n",
    "    # Print summary of corrupted images\n",
    "    if corrupted_test_indices:\n",
    "        print(\"\\nCorrupted image indices:\", corrupted_test_indices)\n",
    "        print(\"These images were marked with prediction value: 0.5\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Use the function\n",
    "predictions_df = predict_and_save(model, test_data, corrupted_test_indices)\n",
    "\n",
    "# Print first few predictions to verify\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0846253-6d2f-4ced-beaa-828cb9c4a26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
